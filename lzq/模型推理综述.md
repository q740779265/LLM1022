# 模型推理综述
## 大方向
<div align="center">
<img src="图片\综述图1.png" alt="综述图1">
</div>
如图 1 所示，我们根据现有工作针对的冗余类型，将其分为三个关键方向：

1. 使长 CoT 变短，专注于使模型在保持性能的同时生成更短的推理路径
2. 构建具有强推理能力的小型语言模型，旨在开发具有强推理能力的紧凑型语言模型
3. 使解码更高效，探索减少解码阶段延迟的策略。



### 模型路由
1. RouteLLM（Ong 等人，2024）使用偏好数据训练路由器，将简单问题分配给轻量级模型，困难问题分配给更强的模型。
2. Sketch-of-Thought（Aytes 等人，2025），将每个输入路由到最合适的推理模式，引入三种启发式模式：概念链（使用最少语言链接想法）、分块符号（将推理组织为符号块）和专家词汇（利用领域特定缩写）。
3. Self-REF（Chuang 等人，2024）添加两个特殊标记（即\<CN>表示自信，\<UN>表示不自信）以指示置信度，在带注释的响应上训练模型进行自我评估。如果不确定，模型会 defer 到更强的模型或弃权。
4. Confident or Seek Stronger（Chuang 等人，2025）进一步分析基于不确定性的路由，观察到不确定性分布在任务间相对稳定，但在模型和不确定性量化（UQ）方法间差异显著。它还设计了一种校准数据构建策略，提高小型语言模型路由决策的可靠性。

### 隐空间推理
1. Disentangling-Memory-and-Reasoning（Jin 等人，2024a）引入显式离散标记如〈memory〉和〈reason〉，使模型能够在潜在空间中将推理分解为独立阶段（即检索相关知识和执行逻辑推理），这种分离促进了更结构化和可解释的推理行为。
2. Token Assorted（Su 等人，2025）通过将文本标记与通过 VQ-VAE获得的潜在标记混合来提高推理效率，在保留关键信息的同时减少序列长度。
3. Filler-Token（Pfau 等人，2024）提出在推理路径中插入无意义的填充标记（如重复的点），允许模型执行额外的隐藏计算，从而提高推理任务的性能。
4. Planning-Token（Wang 等人，2024c）采用一组插入每个推理步骤前的规划标记，引导模型在生成详细解释前生成潜在计划。这些标记通过对推理步骤的隐藏状态进行聚类获得，产生语义有意义且不同的离散表示。
5. Heima（Shen 等人，2025a）向多模态大型语言模型引入思维标记以取代显式推理步骤，实现潜在空间中的推理。
6. Coconut（Hao 等人，2024）通过将最终层隐藏状态反馈到模型中而不解码显式 CoT 标记，在隐藏空间中建模推理，实现更连续高效的推理。这种方法释放了显式 CoT 无法提供的优势，如回溯和并行解码。
7. RELAY（Yu 等人，2025a）遵循这一思路，将循环 Transformer的每次迭代与显式 CoT 步骤对齐，训练后的循环模型用于生成高质量 CoT 链，以在长推理任务上训练更强的自回归模型。
8. Reasoning with Latent Thoughts（Saunshi 等人，2025）证明，多次循环 Transformer 可以模拟更深的模型并自然诱导潜在思维，有效捕捉无标记化步骤的迭代推理。
9. SoftCoT（Xu 等人，2025c）采用CCoT类似策略，训练轻量级辅助模型根据输入生成隐式表示。
10. HCoT（Liu 等人，2024c）采用CCoT类似流程，但更强调压缩过程中的语义对齐。
11. CCoT（Cheng & Van Durme, 2024）训练轻量级 CCOT 模块从输入直接生成压缩的潜在推理标记，然后将其输入解码模块生成简洁答案，
12. LightThinker（Zhang 等人，2025a）提出了一种 CoT 动态压缩策略，将推理链分段并将每个步骤压缩为特殊标记，重点压缩 KV 缓存。这些潜在表示用于后续推理，注意力掩码设计为确保模型只能访问压缩内容而非整个前序步骤。
13. CODI（Shen 等人，2025c）引入了一种新颖的自蒸馏框架，其中共享模型同时作为教师和学生 —— 通过语言建模学习显式 CoT，同时通过对齐答案前标记的隐藏激活学习隐式 CoT。
14. Distill2-to-1（Yu 等人，2024）表明，仅在（输入，答案）对上进行 SFT 即可产生强隐式推理能力。
15. Learning to internalize cot step by step.通过 SFT 逐步移除中间推理步骤，使模型能够在没有显式链的情况下内化推理。
### 模型剪枝
1. Towards reasoning ability of small language models.（Gaurav等人，2025） 系统地探索了剪枝和量化等压缩技术对小型语言模型推理能力的影响，结果表明，虽然量化方法对推理性能的影响最小，但剪枝方法会显著降低推理能力。
2. When reasoning meets compression（Zhang 等人，2025b）对压缩后的 LRMs 在各种推理任务上进行了全面基准测试，发现量化模型保留了较强的推理性能，有时甚至超过原始模型，而激进的剪枝会导致中等稀疏度下的性能崩溃。
### 思维链压缩
1. DAST（Shen 等人，2025b）旨在实现平衡的 CoT（即通过为更具挑战性的问题分配更多推理步骤，为简单问题分配更少步骤来动态调整计算资源）。具体来说，它提出了一个标记长度预算，定义为准确答案中的平均标记数与生成长度预定义上限的加权和，以量化问题难度，惩罚简单问题的过度冗长推理，同时鼓励复杂问题的全面推理
2. THINKPRUNE（Hou 等人，2025）设计了一个长度感知的奖励函数，仅在指定的标记预算内生成正确答案时才提供奖励。该模型使用组相对策略优化（GRPO）算法进行训练，长度约束逐渐收紧。
3. hink When You Need（Yang 等人，2025b）利用成对比较，根据推理的相对长度和准确性生成奖励，引导模型生成简洁准确的解决方案。
4. TALE（Han 等人，2024）进一步探索 DPO 作为替代的微调目标，允许直接控制模型的输出偏好。
### 小模型推理能力探索
1. SLM-Foresee（Srivastava 等人，2025）对各种小型语言模型的推理能力进行了系统研究，表明小型语言模型可以展现出强大的推理潜力。某些模型，如 Qwen2.5 系列，甚至实现了与某些 LLMs 相当或超越的性能。
2. Open-RS（Dang & Ngo，2025）使用 GRPO 算法和从 s1 数据集及 DeepScaleR 数据集中精心挑选的高质量数学推理数据集，增强了小型语言模型的推理能力。他们进一步开发了余弦奖励来有效控制响应长度，在基准测试（如 AIME2024、MATH-500）上的性能达到或超过 o1-preview等模型。
3. SimpleRL-Zoo（Zeng 等人，2025）系统评估了 ZeroRL（即仅使用简单的基于规则的奖励且无需额外监督即可使 LMs 学习长链推理的 RL 范式）的通用性。研究提出了成功进行 ZeroRL 训练的几个关键设计策略：使用简单的基于正确性的奖励、使数据难度与模型容量对齐，以及采用 GRPO 等稳定的 RL 算法。
4. DeepScaleR（Luo 等人，2025b）利用 GRPO 的迭代扩展来延长思考长度（即 8K→16K→24K），显著提高了数学推理基准的性能。15 亿参数的模型 DeepScaleR-1.5B-Preview 超过 O1-Preview，在 AIME 上的 Pass@1 达到 43.1%。

### 采样策略
1. Fast Best-of-N（Sun 等人，2024a）提出推测性拒绝，根据早期阶段的部分奖励停止表现不佳的候选路径。
2. ST-BoN（Wang 等人，2025b）采用早期一致性检查来识别和保留高潜力候选，同时截断其余路径。早期路径评估也可应用于推理数据合成。
3. FastMCTS（Li 等人，2025b）利用蒙特卡洛树搜索（MCTS）构建推理路径，同时在每个步骤评估质量，允许动态路径调整。
4. Non-myopic generation of language models for reasoning and planning.（Ma 等人2024）提出预测解码（Predictive-Decoding），通过模拟几个未来推理步骤（即前瞻轨迹）来重新加权标记分布，缓解 CoT 中标记级生成的短视性。
4. Language models can self-improve at state-value estimation for better search. （Mendes & Ritter2025）从语言模型的逐步生成动态中训练值模型，以估计中间推理状态的效用并决定是否继续。
5. ϕ-Decoding（Xu 等人，2025a）更进一步，在每个步骤模拟多个未来路径，对其进行聚类以形成代表性分布，并从该估计中采样下一步。