# Token Assorted: Mixing Latent and Text Tokens for Improved Language Model Reasoning
## 核心思想
使用VQ-VAE将一组Token（论文中是16个）压缩为一个潜在向量，压缩对象为对训练数据集的CoT前m个token，用新构建的数据集训练模型学会使用压缩Token的能力

## 方法论
1. 使用数据集训练VQ-VAE，让VQ-VAE学会映射Token到固定的码表中（论文中码表大小为1024）
<div align="center">
<img src="图片\论文图2.png" alt="综述图1">
</div>

2. 将码表里的embedding加上\<boLatent>和\<eoLatent> 添加到大模型的词表中

3. 用训练好的VQ-VAE处理训练数据集，只处理CoT的前m个Token，m是不定长的随机数 
<div align="center">
<img src="图片\论文图1.png" alt="综述图1">
</div>

4. 使用处理后训练数据集微调模型

## 结果
该潜在方法在几乎所有任务中始终优于所有基线，无论模型大小如何。对于我们没有观察到改进的任务，该方法也与最佳性能相当。在特定数据集上的增益更为显著，例如 Gaokao-Math-2023。平均而言，8B模型提高了+5.3点，3B模型提高了+2.9点，1B模型提高了+3.7点。